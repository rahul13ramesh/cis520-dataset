{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS520-Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J0AF0sJvRixT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rahul13ramesh/cis520-dataset/blob/master/CIS520_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmR8r-op-R7D",
        "colab_type": "text"
      },
      "source": [
        "# CIS-520 Project -[What's cooking](https://www.kaggle.com/c/whats-cooking-kernels-only)\n",
        "\n",
        "* This notebook looks at the Kaggle contest *What's cooking*. We tackle a supervised and unsupervised learning problem using this dataset\n",
        "* The input features are a collection of words and hence NLP techniques are used to build useful features for the downstream task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UNDnjIe2Qy7H",
        "colab_type": "code",
        "outputId": "1f5b6218-7d6c-4fec-a0be-bc71c6191a2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 606
        }
      },
      "source": [
        "!pip install pywaffle\n",
        "!git clone https://github.com/amueller/word_cloud.git\n",
        "!cd word_cloud && pip install ."
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pywaffle in /usr/local/lib/python3.6/dist-packages (0.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from pywaffle) (3.1.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pywaffle) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pywaffle) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pywaffle) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pywaffle) (1.17.4)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->pywaffle) (2.4.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->pywaffle) (41.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib->pywaffle) (1.12.0)\n",
            "fatal: destination path 'word_cloud' already exists and is not an empty directory.\n",
            "Processing /content/word_cloud\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from wordcloud==1.6.0.post1+g8217e20) (1.17.4)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from wordcloud==1.6.0.post1+g8217e20) (4.3.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from wordcloud==1.6.0.post1+g8217e20) (3.1.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->wordcloud==1.6.0.post1+g8217e20) (0.46)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud==1.6.0.post1+g8217e20) (2.4.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud==1.6.0.post1+g8217e20) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud==1.6.0.post1+g8217e20) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->wordcloud==1.6.0.post1+g8217e20) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib->wordcloud==1.6.0.post1+g8217e20) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud==1.6.0.post1+g8217e20) (41.6.0)\n",
            "Building wheels for collected packages: wordcloud\n",
            "  Building wheel for wordcloud (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordcloud: filename=wordcloud-1.6.0.post1+g8217e20-cp36-cp36m-linux_x86_64.whl size=335578 sha256=0672f379b7013dd61b9bdef70e4f2bf783becd406a5bc9a9f992d00c1272f486\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qb0d3do9/wheels/c9/9e/fe/b14c026a2af072dcf59543bce68dcdfff8a5362e6bb11242d9\n",
            "Successfully built wordcloud\n",
            "Installing collected packages: wordcloud\n",
            "  Found existing installation: wordcloud 1.6.0.post1+g8217e20\n",
            "    Uninstalling wordcloud-1.6.0.post1+g8217e20:\n",
            "      Successfully uninstalled wordcloud-1.6.0.post1+g8217e20\n",
            "Successfully installed wordcloud-1.6.0.post1+g8217e20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vWmY91JiLZ4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import unicodedata\n",
        "import re\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from pywaffle import Waffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3jEKYoZNaHv",
        "colab_type": "code",
        "outputId": "98c95ccd-1cda-479e-b2d9-34d9a59d3883",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "!wget https://raw.githubusercontent.com/rahul13ramesh/cis520-dataset/master/test.json\n",
        "!wget https://raw.githubusercontent.com/rahul13ramesh/cis520-dataset/master/train.json"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-12-01 04:03:43--  https://raw.githubusercontent.com/rahul13ramesh/cis520-dataset/master/test.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2844086 (2.7M) [text/plain]\n",
            "Saving to: ‘test.json.2’\n",
            "\n",
            "\rtest.json.2           0%[                    ]       0  --.-KB/s               \rtest.json.2         100%[===================>]   2.71M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2019-12-01 04:03:44 (47.5 MB/s) - ‘test.json.2’ saved [2844086/2844086]\n",
            "\n",
            "--2019-12-01 04:03:44--  https://raw.githubusercontent.com/rahul13ramesh/cis520-dataset/master/train.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 12415067 (12M) [text/plain]\n",
            "Saving to: ‘train.json.2’\n",
            "\n",
            "train.json.2        100%[===================>]  11.84M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2019-12-01 04:03:45 (129 MB/s) - ‘train.json.2’ saved [12415067/12415067]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zloB0kDdA1k-",
        "colab_type": "text"
      },
      "source": [
        "# Loading dataset\n",
        "We load the dataset and display a single entry from this dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXbqhyiANecj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(\"test.json\", \"r\") as f:\n",
        "    test_data = json.load(f)\n",
        "with open(\"train.json\", \"r\") as f:\n",
        "    train_data = json.load(f)\n",
        "train_data_df = pd.DataFrame(train_data)\n",
        "test_data_df = pd.DataFrame(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1KgSaTuNiGi",
        "colab_type": "code",
        "outputId": "57da0001-c772-4e15-cb89-07a65a9f98e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        }
      },
      "source": [
        "train_data[0]"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cuisine': 'greek',\n",
              " 'id': 10259,\n",
              " 'ingredients': ['romaine lettuce',\n",
              "  'black olives',\n",
              "  'grape tomatoes',\n",
              "  'garlic',\n",
              "  'pepper',\n",
              "  'purple onion',\n",
              "  'seasoning',\n",
              "  'garbanzo beans',\n",
              "  'feta cheese crumbles']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0AF0sJvRixT",
        "colab_type": "text"
      },
      "source": [
        "# Data analysis and Visualization\n",
        "\n",
        "The aim of this analysis is as follows:\n",
        "\n",
        "* Gain an understanding of the dataset, distribution of labels and number of data points. \n",
        "* Look for potentially useful features that can help an classifier achieve higher accuracies.\n",
        "* Look for anamolies in data that need to be handled through data pre-processing and cleaning. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIRJbIDABeHq",
        "colab_type": "text"
      },
      "source": [
        "### Preliminary Understanding of Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCw2sT6MVBOD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_df = pd.DataFrame(train_data)\n",
        "test_data_df = pd.DataFrame(test_data)\n",
        "cuisines = train_data_df[\"cuisine\"].unique()\n",
        "print(\"Data size: \" + str(train_data_df.shape))\n",
        "print(\"Numerb of cuisines: \", str(len(cuisines)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8cwK_MHbY2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_df.groupby(by='cuisine').count().sort_values(by='ingredients',ascending=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bDfxuO9Hu-7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data_df.head())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrZpXP2pHvIR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Waffle Plot of data distribution\n",
        "class_data = train_data_df.cuisine.value_counts()\n",
        "class_data = ((class_data)*100) // class_data.values.sum()\n",
        "colors = [plt.cm.Spectral(i/float(len(cuisines))) for i in range(len(cuisines))]\n",
        "np.random.seed(5)\n",
        "np.random.shuffle(colors)\n",
        "\n",
        "plt.figure(\n",
        "    figsize=(10, 12),\n",
        "    FigureClass=Waffle, \n",
        "    rows=6,\n",
        "    legend={'loc': 'upper left', 'bbox_to_anchor': (1.05, 1), 'fontsize': 9, 'title':'Class', 'ncol': 2},\n",
        "    values=class_data.values,\n",
        "    labels=list(class_data.index),\n",
        "    colors=colors\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEunsCP9tOZC",
        "colab_type": "text"
      },
      "source": [
        "### Analysis of Recipes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NBD_D3AHB2PI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get space seperated text of all the ingredients\n",
        "text = ''\n",
        "for i in range(train_data_df.shape[0]):\n",
        "    text = text + \" \" + ' '.join(train_data_df.iloc[i, 2])\n",
        "\n",
        "# Get all ingredients as a list (along with cuisine, and number of ingredients for each ingredient)\n",
        "text_list = []\n",
        "text_list_cuisine = []\n",
        "text_list_num = []\n",
        "for i in range(train_data_df.shape[0]):\n",
        "    text_list = text_list + train_data_df.iloc[i, 2]\n",
        "    len_ing = len(train_data_df.iloc[i, 2])\n",
        "    text_list_cuisine = text_list_cuisine + [train_data_df.iloc[i, 1] for i in range(len_ing)]\n",
        "    text_list_num = text_list_num + [len_ing for i in range(len_ing)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejBivkR-Cqwa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_ingredients(cuisine_val):\n",
        "    \"\"\"\n",
        "    Get all the ingredients for a particular cuisine\n",
        "    \"\"\"\n",
        "    text = ''\n",
        "    for i in range(train_data_df.shape[0]):\n",
        "        if cuisine_val == train_data_df.iloc[i, 1]:\n",
        "            text = text + \" \" + ' '.join(train_data_df.iloc[i, 2])\n",
        "    return text\n",
        "    \n",
        "def get_num_ingredients(cuisine_val):\n",
        "    \"\"\"\n",
        "    Get the number of ingredients of each recipe for a particular cuisine\n",
        "    \"\"\"\n",
        "    num = []\n",
        "    for i in range(train_data_df.shape[0]):\n",
        "        if cuisine_val == train_data_df.iloc[i, 1]:\n",
        "            num.append(len(train_data_df.iloc[i,2]))\n",
        "    return num\n",
        "\n",
        "def plot_word_cloud(txt, coloc=True):\n",
        "    \"\"\"\n",
        "    Plot the word cloud corresponding to some text\n",
        "    \"\"\"\n",
        "    wordcloud = WordCloud(background_color=\"white\", collocations=coloc).generate(txt)\n",
        "    plt.figure(figsize = (7, 7))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis(\"off\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJ200dr6HJ8_",
        "colab_type": "text"
      },
      "source": [
        "We first plot word clouds based on the ingredients to understand the nature of the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CHBF2-EiRhIp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot word cloud for entire data\n",
        "plot_word_cloud(text,True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJ06hVjcIaWh",
        "colab_type": "text"
      },
      "source": [
        "We next look at the word-clouds in a per cuisine granularity. The visualizations show that the key ingredients in a recipes vary significantly with the cuisine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBTcYtmQD9WR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuisine_ingredients = {}\n",
        "cuisine_numbers = {}\n",
        "for c in cuisines:\n",
        "    cuisine_ingredients[c] = get_ingredients(c)\n",
        "    cuisine_numbers[c] = get_num_ingredients(c)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vn9wwb-bEVtC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_word_cloud(cuisine_ingredients['indian'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzpPZSQTFCQy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_word_cloud(cuisine_ingredients['italian'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ekuzq5s6FMaW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_word_cloud(cuisine_ingredients['mexican'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "siDJ_gJTIBUI",
        "colab_type": "text"
      },
      "source": [
        "Next we look use Violin-plots (box plots) to understand how  the number of ingredients varies with cuisine. The plots indicate that this feature can be used a useful feature for a classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77Ro61SDFnLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cuisine_num_array = []\n",
        "cuisine_val = []\n",
        "for c in cuisines:\n",
        "    cuisine_num_array = cuisine_num_array + cuisine_numbers[c]\n",
        "    cuisine_val = cuisine_val + [c for _ in range(len(cuisine_numbers[c]))]\n",
        "cuisine_number_df = pd.DataFrame([cuisine_num_array, cuisine_val]).T\n",
        "cuisine_number_df.columns = [\"Number of Ingredients\", \"Cuisine\"]\n",
        "cuisine_number_df[\"Number of Ingredients\"] = pd.to_numeric(cuisine_number_df[\"Number of Ingredients\"])\n",
        "\n",
        "plt.figure(figsize=(18,5))\n",
        "sns.violinplot(x=\"Cuisine\", y=\"Number of Ingredients\", data=cuisine_number_df)\n",
        "\n",
        "plt.figure(figsize=(18,5))\n",
        "sns.violinplot(x=\"Cuisine\", y=\"Number of Ingredients\", data=cuisine_number_df[cuisine_number_df[\"Number of Ingredients\"] <= 25])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIacPYavemOs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "long_ingredients = []\n",
        "small_ingredients1 = []\n",
        "small_ingredients2 = []\n",
        "small_ingredients3 = []\n",
        "for i in range(train_data_df.shape[0]):\n",
        "    ing =  train_data_df.iloc[i, 2]\n",
        "    if len(ing) >= 30:\n",
        "        long_ingredients.append((ing, train_data_df.iloc[i, 1]))\n",
        "    elif len(ing) == 1:\n",
        "        small_ingredients1.append((ing, train_data_df.iloc[i, 1]))\n",
        "    elif len(ing) == 2:\n",
        "        small_ingredients2.append((ing, train_data_df.iloc[i, 1]))\n",
        "    elif len(ing) == 3:\n",
        "        small_ingredients3.append((ing, train_data_df.iloc[i, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N4ItM-vagPcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Number of long ingredients: \" + str(len(long_ingredients)))\n",
        "print(\"Number of short ingredients: \" + str(len(small_ingredients1)))\n",
        "print(\"Number of short ingredients: \" + str(len(small_ingredients2)))\n",
        "print(\"Number of short ingredients: \" + str(len(small_ingredients3)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abZQCPNsg_16",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_ingredients1\n",
        "# Remove elements with just 1 ingredients, clearly"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKHxh9iLhY27",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_ingredients2[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v_b6QJ4hZDA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "small_ingredients3[0:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vt6F55L0iOSf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(long_ingredients[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDCl1b85tqY-",
        "colab_type": "text"
      },
      "source": [
        "### Understanding Individual Ingredient / Tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0sS8a_By1F7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_analysis = text.lower()\n",
        "# Remove elements\n",
        "rem_elems = [\"lb.\", \"oz.\", \"inch\", \"%\", \"™\", \"®\",  \"€\", \"(\", \")\", \",\"]\n",
        "for e in rem_elems:\n",
        "    text_analysis = text_analysis.replace(e, \" \")\n",
        "text_analysis = text_analysis.replace(\"’\", \"'\")\n",
        "text_analysis = ''.join([i for i in text_analysis if not i.isdigit()])\n",
        "ing_analysis = text_analysis.split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HMU3oJ_3pv2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_counts = []\n",
        "print(\"Number of Ingredients: \" + str(len(set(ing_analysis))))\n",
        "for x in set(ing_analysis):\n",
        "    word_counts.append((ing_analysis.count(x), x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghpe2UrP5ltC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sorted(word_counts, reverse=True)[0:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HyB2V3kS6FQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ing_count_df = pd.DataFrame(word_counts)\n",
        "ing_count_df.columns = [\"Count\", \"Ingredient\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5w3YSvY77-K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sns.countplot(data=ing_count_df[ing_count_df.Count <= 30], x=\"Count\")\n",
        "# About 600 entries"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uS2EF8gfA-u7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  sns.violinplot(data=ing_count_df[(ing_count_df[\"Count\"] >= 100)], x=\"Count\", cut=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEDn4DiLtGNN",
        "colab_type": "text"
      },
      "source": [
        "### Understanding character Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k_bHh4nSHQz7",
        "colab_type": "text"
      },
      "source": [
        "We now look to get an understanding of the frequency of occurence of words in an ingredient. In particular we would like to look at ingredients with special characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfWpoBn4HWHh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(' '.join(sorted(set(text))))\n",
        "# Convert upper case to lower case"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gP3FEY4cp3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "digit_char = []\n",
        "\n",
        "for ing, num, cuisine  in zip(text_list, text_list_num, text_list_cuisine):\n",
        "    if any(char.isdigit() for char in ing):\n",
        "        count += 1\n",
        "        if count <= 10:\n",
        "            print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "        digit_char.append([num, cuisine, ing])\n",
        "# remove numbers number/number, number%, num.number\n",
        "#  add presense of number as feature?\n",
        "tmp_text = ' '.join([row[2] for row in digit_char])\n",
        "plot_word_cloud(tmp_text, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fclTH0LN7oPJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for ing, num, cuisine  in zip(text_list, text_list_num, text_list_cuisine):\n",
        "    if \"€\" in ing:\n",
        "        print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "# remove the pound character, since only one item has character"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvAcoMqg8Hb1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count = 0\n",
        "copyright_char = []\n",
        "\n",
        "for ing, num, cuisine  in zip(text_list, text_list_num, text_list_cuisine):\n",
        "    if \"®\" in ing or (\"™\" in ing):\n",
        "        count += 1\n",
        "        if count <= 10:\n",
        "            print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "        copyright_char.append([num, cuisine, ing])\n",
        "print(len(copyright_char))\n",
        "tmp_text = \"\"\n",
        "for row in copyright_char:\n",
        "    for w in row[2].split():\n",
        "        if (\"®\" in w )or (\"™\" in w):\n",
        "            tmp_text = tmp_text + \" \" + w \n",
        "plot_word_cloud(tmp_text,False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJh2lWWq8wAU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count1 = 0\n",
        "count2 = 0\n",
        "spl_char = []\n",
        "\n",
        "for ing, num, cuisine  in zip(text_list, text_list_num, text_list_cuisine):\n",
        "    if (\"!\" in ing) or (\"&\" in ing) or (\"(\" in ing) or (\")\" in ing) or (\"'\" in ing) or (\"%\" in ing):\n",
        "        if count1 <= 5 and ('!' in ing):\n",
        "            count1 += 1\n",
        "            print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "        spl_char.append([num, cuisine, ing])\n",
        "\n",
        "        if count2 <= 5 and ('&' in ing):\n",
        "            count2 += 1\n",
        "            print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "        spl_char.append([num, cuisine, ing])\n",
        "\n",
        "tmp_text = \"\"\n",
        "for row in spl_char:\n",
        "    for w in row[2].split():\n",
        "        if (\"!\" in w) or (\"&\" in w) or (\"(\" in w) or (\")\" in w) or (\"'\" in w) or (\"%\" in w):\n",
        "            tmp_text = tmp_text + \" \" + w \n",
        "plot_word_cloud(tmp_text, False)\n",
        "# replace & with and maybe (half & half)\n",
        "# Remove %, (, )\n",
        "# ' occurs for plural\n",
        "\n",
        "#There are some spelling errors for example\n",
        "#believ as seen below\n",
        "# ! occurs for a brand \"I can't Believe ti's Not Butter\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77WD6gB6e_cN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "accented_char_set= [\"â\", \"ç\", \"è\", \"é\", \"í\", \"î\", \"ú\", \"’\"]\n",
        "accented_char = []\n",
        "count1 = 0\n",
        "count2 = 0\n",
        "\n",
        "for ing, num, cuisine  in zip(text_list, text_list_num, text_list_cuisine):\n",
        "    if 1 in [c in ing for c in accented_char_set]:\n",
        "        if count1 <= 5 and '’' in ing:\n",
        "            count1 += 1\n",
        "            print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "        elif count2 <= 5:\n",
        "            count2 += 1\n",
        "            print(str(num) + \"\\t\" + cuisine + \"\\t\\t\" + ing)\n",
        "        accented_char.append([num, cuisine, ing])\n",
        "    \n",
        "tmp_text = \"\"\n",
        "for row in accented_char:\n",
        "    for w in row[2].split():\n",
        "        if 1 in [c in w for c in accented_char_set]:\n",
        "            tmp_text = tmp_text + \" \" + w \n",
        "plot_word_cloud(tmp_text, False)\n",
        "# '’' Replace with apostrophe '"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHoZfIohurv2",
        "colab_type": "text"
      },
      "source": [
        "# Creating Features / Dataset Variants"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlrn032qvnZh",
        "colab_type": "text"
      },
      "source": [
        "https://www.kaggle.com/rejasupotaro/what-are-ingredients\n",
        "\n",
        "* Make Upper case to lower case\n",
        "* Remove elements with 1 ingredient\n",
        "* Add feature for special characters â ç è é í î ú \n",
        "* Remove lb. inch oz.\n",
        "* Add feature for:  ™ ® \n",
        "* Add feature for: !     (! used for a particular brand)\n",
        "* Add feature for: Numbers/percentages\n",
        "* Remove symbol: € ™ ( ) ® . - %\n",
        "* Change ’ with ' (first one is a special character)\n",
        "* Replace \"&\" with and  or space(not sure about this)\n",
        "* Remove all numbers\n",
        "* Remove all ,\n",
        "* Add feature if ™ ® \n",
        "* If there are many ingredients, maybe pick a subset of ingredients as features?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD9PPG6BuoQs",
        "colab_type": "text"
      },
      "source": [
        "# TODO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vreuxhhiLeg6",
        "colab_type": "text"
      },
      "source": [
        "* Look at the common words in each cuisine (see most overlapping words too)\n",
        "* LDA\n",
        "* K-means\n",
        "* A number of words occur only one, what to do about these words?\n",
        "* Choice of features (run evaluation)/different subsets - on subset\n",
        "* Model hyper-paramters (run grid-search) - on subset\n",
        "\n",
        "## Sanity checks after preprocessing\n",
        "\n",
        "\n",
        "\n",
        "1.   Remove Empty recepies: Done\n",
        "2.   Remove Recepies with one or two ingredients: Done\n",
        "4.   Join words like: Olive oil -> Olive_oil: TODO\n",
        "5.   Join words like: warm water, cold water, lukewarm water, boiling water, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSY-CK3ncCqS",
        "colab_type": "text"
      },
      "source": [
        "Let us analyse the dictionary of characters that are present"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRMcR9fDL1O3",
        "colab_type": "text"
      },
      "source": [
        "* Token/lemmatizing/capitalization, synonym identification, accented characters and number of ingredients"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2s-DO5M-vHV",
        "colab_type": "text"
      },
      "source": [
        "# Pre-processing\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWH2oN3v-x2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_train_test_data(train_data_df, test_data_df):\n",
        "  train_recepies = [] \n",
        "  train_targets = []\n",
        "  for ig_item in train_data_df['ingredients']:\n",
        "      train_recepies.append(ig_item)\n",
        "  for cuisine_item in train_data_df['cuisine']:\n",
        "      train_targets.append(cuisine_item)\n",
        "\n",
        "  # create test data only, no targets\n",
        "  test_recepies = []\n",
        "  for ig_item in test_data_df['ingredients']:\n",
        "      test_recepies.append(ig_item)\n",
        "  return train_recepies, train_targets, test_recepies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OmaOocmd_6sI",
        "colab_type": "code",
        "outputId": "8f9ba1db-cfb8-4ddf-92a5-595cb99f0608",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_features, train_targets, test_features = create_train_test_data(train_data_df, test_data_df)\n",
        "print(train_targets[0:5])"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['greek', 'southern_us', 'filipino', 'indian', 'indian']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poPcpASERePE",
        "colab_type": "text"
      },
      "source": [
        "#### Remove small receipies, special characters, quantity classifiers (inch, oz etc.)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwDtfaw4cfZL",
        "colab_type": "code",
        "outputId": "92b2390a-5e57-4293-b464-b84533ce7592",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "def process_features(features, targets, mode=\"train\"):\n",
        "    processed_recipe = []\n",
        "    processed_cuisine = []\n",
        "    count_len = 0\n",
        "\n",
        "    for idx, recipe in enumerate(features):\n",
        "        processed_ingredient = []\n",
        "        if mode=='train':\n",
        "            cuisine_lower = targets[idx].lower()\n",
        "\n",
        "        # Remove certain words\n",
        "        cur_features = {}\n",
        "        ingredient_all = ' '.join(recipe)\n",
        "\n",
        "        # f1) Add feature for number of ingredients\n",
        "        if len(recipe) <= 2:\n",
        "            count_len += 1\n",
        "            continue\n",
        "        cur_features[\"num_ingredients\"] = len(recipe)\n",
        "\n",
        "        # f2) Change accented characters to ascii characters\n",
        "        accented_char_set= [\"â\", \"ç\", \"è\", \"é\", \"í\", \"î\", \"ú\", \"’\"]\n",
        "        cur_features[\"accented\"] = 0\n",
        "        if 1 in [c in ingredient_all for c in accented_char_set]:\n",
        "            cur_features[\"accented\"] = 1\n",
        "\n",
        "        # f3) Add feature for measurement units\n",
        "        measurement_words = [\"pound\", \"kg\", \"lb\", \"oz\", \"ounc\", \"inch\"]\n",
        "        cur_features[\"measurement\"] = 0\n",
        "        if np.any([w in ingredient_all for w in measurement_words]):\n",
        "            cur_features[\"measurement\"] = 1\n",
        "\n",
        "        # f4) Symbols\n",
        "        cur_features[\"symbol\"] = 0\n",
        "        if (\"™\" in ingredient_all) or (\"®\" in ingredient_all) or (\"!\" in ingredient_all):\n",
        "            cur_features[\"symbol\"] = 1\n",
        "\n",
        "        # f5) Feature for numbers\n",
        "        cur_features[\"number\"] = 0\n",
        "        if any(char.isdigit() for char in ingredient_all):\n",
        "            cur_features[\"number\"] = 1\n",
        "            \n",
        "        # f5) Feature for percentage\n",
        "        cur_features[\"percentage\"] = 0\n",
        "        if \"%\" in ingredient_all:\n",
        "            cur_features[\"percentage\"] = 1\n",
        "\n",
        "        # p1) Convert to lower case\n",
        "        ingredient_all = ingredient_all.lower()\n",
        "        # p2) Change accented characters to ascii characters\n",
        "        ingredient_all = unicodedata.normalize('NFD', ingredient_all).encode('ascii', 'ignore')\n",
        "        ingredient_all = ingredient_all.decode('ascii')\n",
        "        # p3) Remove measurement units\n",
        "        ingredient_all = re.sub((r'\\b(pound|kg|lb|oz|ounc|inch)\\b'), ' ', ingredient_all) \n",
        "        # p4) Remove symbol\n",
        "        ingredient_all = ingredient_all.replace(\"!\", \" \")\n",
        "        ingredient_all = ingredient_all.replace(\"™\", \" \")\n",
        "        ingredient_all = ingredient_all.replace(\"®\", \" \")\n",
        "        # p5) Remove brackets\n",
        "        ingredient_all = ingredient_all.replace(\"(\", \" \")\n",
        "        ingredient_all = ingredient_all.replace(\")\", \" \")\n",
        "        # p6) Remove hyphens\n",
        "        ingredient_all = ingredient_all.replace(\"-\", \" \")\n",
        "        # p7) Remove numbers\n",
        "        ingredient_all = re.sub(\"\\d\", \" \", ingredient_all)\n",
        "        # p8) Remove percentages and / (occur with numbers)\n",
        "        ingredient_all = ingredient_all.replace(\"/\", \" \")\n",
        "        ingredient_all = ingredient_all.replace(\"%\", \" \")\n",
        "\n",
        "        # p9) Remove some more characters\n",
        "        ingredient_all = ingredient_all.replace(\".\", \" \")\n",
        "        ingredient_all = ingredient_all.replace(\"€\", \" \")\n",
        "        ingredient_all = ingredient_all.replace(\",\", \" \")\n",
        "\n",
        "        # p10) Handle apostrophe\n",
        "        ingredient_all = ingredient_all.replace(\"’\", \"'\")\n",
        "        ingredient_all = ingredient_all.replace(\"'\", \"\")\n",
        "\n",
        "        # p11) Replace & with and\n",
        "        ingredient_all = ingredient_all.replace(\"&\", \"and\")\n",
        "\n",
        "        ings = ' '.join(ingredient_all.split())\n",
        "        cur_features[\"num_words\"] = len(ingredient_all.split())\n",
        "\n",
        "        processed_recipe.append((\n",
        "                cur_features[\"num_words\"], cur_features[\"num_ingredients\"],\n",
        "                cur_features[\"accented\"], cur_features[\"measurement\"],\n",
        "                cur_features[\"symbol\"], cur_features[\"number\"],\n",
        "                cur_features[\"percentage\"], ings\n",
        "            ))\n",
        "        if mode=='train':\n",
        "            processed_cuisine.append(cuisine_lower)\n",
        "    \n",
        "    print('Removed {} small recipe from input data'.format(count_len))\n",
        "    if mode=='train':\n",
        "        return processed_recipe, processed_cuisine\n",
        "    else:\n",
        "        return processed_recipe\n",
        "\n",
        "processed_train_features, processed_train_target = process_features(train_features, train_targets, mode='train')\n",
        "processed_test_features = process_features(test_features, train_targets,  mode='test') # just passing the targets, not using it while preprocessing test data\n",
        "\n",
        "\n",
        "print('Sanity check --------------')\n",
        "print(len(processed_train_target))\n",
        "print(len(processed_train_features))"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Removed 215 small recipe from input data\n",
            "Removed 56 small recipe from input data\n",
            "Sanity check --------------\n",
            "39559\n",
            "39559\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L0KuUyO43cHv",
        "colab_type": "code",
        "outputId": "717938ad-91e4-45ac-b2f1-8d4233127eee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# The only characters that remain in representation are characters\n",
        "txt = []\n",
        "for i in range(len(processed_train_features)):\n",
        "    txt.append(processed_train_features[i][-1])\n",
        "print(' '.join(sorted(set(' '.join(txt)))))"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  a b c d e f g h i j k l m n o p q r s t u v w x y z\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv9k_ToqE3FD",
        "colab_type": "code",
        "outputId": "97e3c13b-73a6-415c-aff3-cb83ae5c5e12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_data = pd.DataFrame(processed_train_features)\n",
        "test_data = pd.DataFrame(processed_test_features)\n",
        "\n",
        "train_data.columns = [\"num_words\", \"num_ingredients\", \"accented\", \"measurement\", \"symbol\", \"number\", \"percentage\", \"text\"]\n",
        "print(train_data.shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39559, 8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kXyGEczRRjT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "outputId": "b7a8380c-91a9-4ceb-b643-26d36a689049"
      },
      "source": [
        "train_data.describe()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>num_words</th>\n",
              "      <th>num_ingredients</th>\n",
              "      <th>accented</th>\n",
              "      <th>measurement</th>\n",
              "      <th>symbol</th>\n",
              "      <th>number</th>\n",
              "      <th>percentage</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>39559.000000</td>\n",
              "      <td>39559.000000</td>\n",
              "      <td>39559.000000</td>\n",
              "      <td>39559.000000</td>\n",
              "      <td>39559.000000</td>\n",
              "      <td>39559.00000</td>\n",
              "      <td>39559.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>20.673955</td>\n",
              "      <td>10.815921</td>\n",
              "      <td>0.014586</td>\n",
              "      <td>0.084709</td>\n",
              "      <td>0.004651</td>\n",
              "      <td>0.00905</td>\n",
              "      <td>0.007938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>8.919131</td>\n",
              "      <td>4.392268</td>\n",
              "      <td>0.119889</td>\n",
              "      <td>0.278452</td>\n",
              "      <td>0.068042</td>\n",
              "      <td>0.09470</td>\n",
              "      <td>0.088740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>3.000000</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>14.000000</td>\n",
              "      <td>8.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>20.000000</td>\n",
              "      <td>10.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>26.000000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>141.000000</td>\n",
              "      <td>65.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          num_words  num_ingredients  ...       number    percentage\n",
              "count  39559.000000     39559.000000  ...  39559.00000  39559.000000\n",
              "mean      20.673955        10.815921  ...      0.00905      0.007938\n",
              "std        8.919131         4.392268  ...      0.09470      0.088740\n",
              "min        3.000000         3.000000  ...      0.00000      0.000000\n",
              "25%       14.000000         8.000000  ...      0.00000      0.000000\n",
              "50%       20.000000        10.000000  ...      0.00000      0.000000\n",
              "75%       26.000000        13.000000  ...      0.00000      0.000000\n",
              "max      141.000000        65.000000  ...      1.00000      1.000000\n",
              "\n",
              "[8 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGIZrd1ZR30H",
        "colab_type": "text"
      },
      "source": [
        "## Feature engineering: Binarize textual recepies/features\n",
        "\n",
        "max_df = 0.99 -> remove any word which is occuring in more than 99% of the sample\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzLK5VWaHqm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "# strip_accents uses ascii\n",
        "def generate_bag_of_words(train_bin_vec, test_bin_vec):\n",
        "    binarizer = CountVectorizer(analyzer = \"word\",  ngram_range = (1, 2), \n",
        "                                binary = False, tokenizer = None, preprocessor = None, \n",
        "                                stop_words = None,  min_df=0.01) \n",
        "    binarizer.fit([str(i[-1]) for i in train_bin_vec])\n",
        "    binary_vector_tr = binarizer.transform([str(i[-1]) for i in train_bin_vec])\n",
        "    binary_vector_te = binarizer.transform([str(i[-1]) for i in test_bin_vec])\n",
        "    return binary_vector_tr, binary_vector_te, binarizer\n",
        "\n",
        "def generate_tf_idf(train_bin_vec, test_bin_vec, bin=False, ngrammax=2,\n",
        "                    use_idf=True, smooth_idf=True, sublinear_tf=False):\n",
        "    vectorizer = TfidfVectorizer(input=\"content\", preprocessor=None, tokenizer=None, \n",
        "                                analyzer=\"word\", ngram_range=(1, ngrammax), min_df=0.01,\n",
        "                                binary=bin, norm=\"l2\", use_idf=use_idf, smooth_idf=smooth_idf,\n",
        "                                sublinear_tf=sublinear_tf)\n",
        "    vectorizer.fit([str(i[-1]) for i in train_bin_vec])\n",
        "    binary_vector_tr = vectorizer.transform([str(i[-1]) for i in train_bin_vec])\n",
        "    binary_vector_te = vectorizer.transform([str(i[-1]) for i in test_bin_vec])\n",
        "    return binary_vector_tr, binary_vector_te, vectorizer\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q3rJCExSEtS",
        "colab_type": "code",
        "outputId": "b5bedde2-9a58-4b29-a75a-fd8a104c080e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_rep, test_rep, binarizer = generate_bag_of_words(processed_train_features, processed_test_features)\n",
        "print(train_rep.shape)"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(39559, 513)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WFzz_80NcJo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "e03a179e-8e59-4732-d219-ad1ac1930d55"
      },
      "source": [
        "sorted(list(binarizer.vocabulary_.keys()))[0:10]"
      ],
      "execution_count": 144,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['all',\n",
              " 'all purpose',\n",
              " 'allspice',\n",
              " 'almonds',\n",
              " 'and',\n",
              " 'apple',\n",
              " 'avocado',\n",
              " 'baby',\n",
              " 'bacon',\n",
              " 'baking']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMUUnoaBSLvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "def train(train_binary_vector, processed_train_target, cross_val=True, confusion=True):\n",
        "    lb_en = LabelEncoder()\n",
        "    processed_train_target_encoded = lb_en.fit_transform(processed_train_target)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_binary_vector, processed_train_target_encoded , random_state = 0)\n",
        "    classifiers = []\n",
        "    \n",
        "    log_reg = LogisticRegression(solver='lbfgs', multi_class='auto', max_iter=1000)\n",
        "    random_fc = RandomForestClassifier(n_estimators=20, random_state=42)\n",
        "    \n",
        "    # classifiers = {\"Logistic\" :log_reg, \"Random_Forest\": random_fc}\n",
        "    classifiers = {\"Logistic\" :log_reg}\n",
        "\n",
        "\n",
        "    for cl in classifiers:\n",
        "        print(cl)\n",
        "        mod = classifiers[cl]\n",
        "    \n",
        "        X , X_test, y, y_test = train_test_split(X_train, y_train, test_size=0.20)\n",
        "    \n",
        "        mod.fit(X, y)\n",
        "        print('RFC LogLoss {score}'.format(score=log_loss(y, mod.predict_proba(X))))\n",
        "        print('RFC Accuracy {score}'.format(score=accuracy_score(y, mod.predict(X))))\n",
        "        print('RFC Accuracy {score}'.format(score=accuracy_score(y_test, mod.predict(X_test))))\n",
        "    \n",
        "        if confusion:\n",
        "            y_pred = mod.predict(X_test)\n",
        "            conf_mat = confusion_matrix(y_test, y_pred)\n",
        "            plt.clf()\n",
        "            sns.heatmap(conf_mat, annot=False)\n",
        "            plt.show()\n",
        "    \n",
        "        if cross_val:\n",
        "            scores = cross_val_score(mod, X_train, y_train, cv=5)\n",
        "            print(scores)\n",
        "            print(np.mean(scores))\n",
        "        print(\"-----\")\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CodWLa8OXF60",
        "colab_type": "code",
        "outputId": "a007ef0e-a230-4f90-970f-eae0d16ac3ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "train(train_rep1, processed_train_target)"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic\n",
            "RFC LogLoss 0.6891058691630926\n",
            "RFC Accuracy 0.7878660206446176\n",
            "RFC Accuracy 0.720593191776205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWMAAAD8CAYAAACihcXDAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[0.71746085 0.71830274 0.7333221  0.72445194 0.72185654]\n",
            "0.7230788356734159\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvDHKIRPzr3f",
        "colab_type": "text"
      },
      "source": [
        "# Experimenting with Variants of Feature Generation\n",
        "\n",
        "* TF-IDF\n",
        "* Lemmatization (inspired by : https://www.kaggle.com/longyg/svc-classification)\n",
        "* Word2Vec\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sVj3t4gL0aKJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 770
        },
        "outputId": "fa0a9fc8-cf53-4c78-ef2a-f34a1a512854"
      },
      "source": [
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2, use_idf=True, smooth_idf=True,\n",
        "                                                 sublinear_tf=False)\n",
        "train(train_rep, processed_train_target, False, False)\n",
        "\n",
        "# Varying n-grams\n",
        "print(\"1-gram\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=1,\n",
        "                                                 use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "train(train_rep, processed_train_target, False, False)\n",
        "\n",
        "print(\"3-gram\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=3,\n",
        "                                                 use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "train(train_rep, processed_train_target, False, False)\n",
        "\n",
        "# Varying n-grams\n",
        "print(\"binary\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=True, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
        "train(train_rep, processed_train_target, False, False)\n",
        "\n",
        "print(\"Non-smooth IDF\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=False)\n",
        "train(train_rep, processed_train_target, False, False)\n",
        "\n",
        "print(\"Sublinear_TF\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=True, sublinear_tf=True)\n",
        "train(train_rep, processed_train_target, False, False)\n",
        "\n",
        "print(\"No-DF\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=False, smooth_idf=True, sublinear_tf=False)\n",
        "train(train_rep, processed_train_target, False, False)"
      ],
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic\n",
            "RFC LogLoss 0.8616560085433441\n",
            "RFC Accuracy 0.7534442805982726\n",
            "RFC Accuracy 0.7219413549039434\n",
            "-----\n",
            "1-gram\n",
            "Logistic\n",
            "RFC LogLoss 0.8848051777134502\n",
            "RFC Accuracy 0.739709290077944\n",
            "RFC Accuracy 0.7253117627232896\n",
            "-----\n",
            "3-gram\n",
            "Logistic\n",
            "RFC LogLoss 0.8632833899999495\n",
            "RFC Accuracy 0.7507057088687592\n",
            "RFC Accuracy 0.7180653859116953\n",
            "-----\n",
            "binary\n",
            "Logistic\n",
            "RFC LogLoss 0.8591607509394944\n",
            "RFC Accuracy 0.7520539287971351\n",
            "RFC Accuracy 0.7263228850690934\n",
            "-----\n",
            "Non-smooth IDF\n",
            "Logistic\n",
            "RFC LogLoss 0.8554747670276479\n",
            "RFC Accuracy 0.7539498630714135\n",
            "RFC Accuracy 0.7126727334007414\n",
            "-----\n",
            "Sublinear_TF\n",
            "Logistic\n",
            "RFC LogLoss 0.858280091617357\n",
            "RFC Accuracy 0.7520960606698968\n",
            "RFC Accuracy 0.7229524772497472\n",
            "-----\n",
            "No-DF\n",
            "Logistic\n",
            "RFC LogLoss 0.9032963709511864\n",
            "RFC Accuracy 0.7411417737518433\n",
            "RFC Accuracy 0.7221098752949107\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rf4KaEefCbx9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "64a7dea4-7d15-4c44-9704-edf2d9be3036"
      },
      "source": [
        "print(\"Heursitic best\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "train(train_rep, processed_train_target, True, False)"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Heursitic best\n",
            "Logistic\n",
            "RFC LogLoss 0.8576925926689545\n",
            "RFC Accuracy 0.7532757531072256\n",
            "RFC Accuracy 0.7210987529491069\n",
            "[0.71931302 0.71998653 0.73113208 0.72310287 0.71881857]\n",
            "0.7224706106059282\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2FgkSzMJkrg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "af3188fb-2926-4ebf-c85a-adbd614e594c"
      },
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 149
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of2l_F-cJqxF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def stem_vec(train_bin_vec, stemtype=0):\n",
        "    if stemtype == 0:\n",
        "        stemmer = PorterStemmer()\n",
        "    else:\n",
        "        stemmer = LancasterStemmer()\n",
        "\n",
        "    stem_dat = []\n",
        "    for dat in train_bin_vec:\n",
        "        stem_txt = []\n",
        "        for w in dat[-1].split():\n",
        "            stem_txt.append(stemmer.stem(w))\n",
        "        stem_dat.append([' '.join(stem_txt)])\n",
        "    return stem_dat\n",
        "\n",
        "def lemmatize_vec(train_bin_vec):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lem_dat = []\n",
        "    for dat in train_bin_vec:\n",
        "        stem_txt = []\n",
        "        for w in dat[-1].split():\n",
        "            stem_txt.append(lemmatizer.lemmatize(w))\n",
        "        lem_dat.append([' '.join(stem_txt)])\n",
        "    return lem_dat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOQkqVlccx36",
        "colab_type": "text"
      },
      "source": [
        "Tokenization and Lemmetizing doesn't really affect performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrG1d0SvPUGV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "outputId": "35813f0a-4981-4572-d0e8-d2ae46c1623d"
      },
      "source": [
        "print(\"Stemming Porter\")\n",
        "stemmed_dat = stem_vec(processed_train_features, 0)\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(stemmed_dat, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "train(train_rep, processed_train_target, True, False)\n",
        "\n",
        "print(\"Stemming Lancaster\")\n",
        "stemmed_dat = stem_vec(processed_train_features, 1)\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(stemmed_dat, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "train(train_rep, processed_train_target, True, False)\n",
        "\n",
        "print(\"Lemmatize\")\n",
        "lemmed_dat = lemmatize_vec(processed_train_features)\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(stemmed_dat, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "train(train_rep, processed_train_target, True, False)"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Stemming Porter\n",
            "Logistic\n",
            "RFC LogLoss 0.8482931617382061\n",
            "RFC Accuracy 0.7546239730356015\n",
            "RFC Accuracy 0.723963599595551\n",
            "[0.71796599 0.72419599 0.73365903 0.72023609 0.72253165]\n",
            "0.7237177486080528\n",
            "-----\n",
            "Stemming Lancaster\n",
            "Logistic\n",
            "RFC LogLoss 0.8445472719989728\n",
            "RFC Accuracy 0.7579523909837792\n",
            "RFC Accuracy 0.7165487023929895\n",
            "[0.71796599 0.72604816 0.73433288 0.72192243 0.72067511]\n",
            "0.7241889123416083\n",
            "-----\n",
            "Lemmatize\n",
            "Logistic\n",
            "RFC LogLoss 0.8427042074075547\n",
            "RFC Accuracy 0.7562671160733094\n",
            "RFC Accuracy 0.7108190091001011\n",
            "[0.71796599 0.72604816 0.73433288 0.72192243 0.72067511]\n",
            "0.7241889123416083\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4gDNN7KZI4q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Heursitic best\")\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "train(train_rep, processed_train_target, True, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvUKbVchdll8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528
        },
        "outputId": "ef4aa4b7-df88-406a-ce05-1cff347b325c"
      },
      "source": [
        "def merge_representation(train_features, vectorized_text):\n",
        "    merged_dat = []\n",
        "    for i in range(len(train_features)):\n",
        "        merged_dat.append(np.concatenate((np.array(train_features[0][:-1]), vectorized_text[i])))\n",
        "    return np.array(merged_dat)\n",
        "\n",
        "train_rep, test_rep, binarizer = generate_tf_idf(processed_train_features, processed_test_features,\n",
        "                                                 bin=False, ngrammax=2,\n",
        "                                                 use_idf=True, smooth_idf=False, sublinear_tf=True)\n",
        "\n",
        "train_rep = merge_representation(processed_train_features, train_rep.toarray())\n",
        "train(train_rep, processed_train_target, True, True)"
      ],
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Logistic\n",
            "RFC LogLoss 0.8548341431293242\n",
            "RFC Accuracy 0.7520539287971351\n",
            "RFC Accuracy 0.7234580384226491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWkAAAD8CAYAAAC1p1UKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1gAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
            "  \"of iterations.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[0.7186395  0.72032329 0.73113208 0.72411467 0.71848101]\n",
            "0.7225381095283416\n",
            "-----\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWW4rc5tcPHZ",
        "colab_type": "text"
      },
      "source": [
        "* Compare different models (search hyper-params)\n",
        "* Dimensionality reduction and different clustering methods"
      ]
    }
  ]
}